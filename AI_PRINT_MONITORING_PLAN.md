# 3D í”„ë¦°í„° ì¶œë ¥ ë¶ˆëŸ‰ ê°ì§€ AI ì‹œìŠ¤í…œ ê°œë°œ ê³„íš

## ğŸ“‹ ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)
4. [ê°œë°œ ë‹¨ê³„ë³„ ê³„íš](#ê°œë°œ-ë‹¨ê³„ë³„-ê³„íš)
5. [ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµ](#ë°ì´í„°-ìˆ˜ì§‘-ë°-í•™ìŠµ)
6. [API ì„¤ê³„](#api-ì„¤ê³„)
7. [í”„ë¡ íŠ¸ì—”ë“œ í†µí•©](#í”„ë¡ íŠ¸ì—”ë“œ-í†µí•©)
8. [ë°°í¬ ë° ìš´ì˜](#ë°°í¬-ë°-ìš´ì˜)

---

## í”„ë¡œì íŠ¸ ê°œìš”

### ëª©í‘œ
WebRTC ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë°ì„ í™œìš©í•˜ì—¬ 3D í”„ë¦°í„° ì¶œë ¥ ê³¼ì •ì„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§í•˜ê³ , AI ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë¶ˆëŸ‰ì„ ê°ì§€:
- **ìŠ¤íŒŒê²Œí‹°í™” (Spaghetti)**: í•„ë¼ë©˜íŠ¸ê°€ ì—‰ì¼œì„œ ì¶œë ¥ë˜ëŠ” í˜„ìƒ
- **ë ˆì´ì–´ ë¶„ë¦¬ (Layer Separation)**: ë ˆì´ì–´ ê°„ ì ‘ì°© ë¶ˆëŸ‰
- **ì™€í•‘ (Warping)**: ì¶œë ¥ë¬¼ ëª¨ì„œë¦¬ê°€ ë“¤ëœ¨ëŠ” í˜„ìƒ
- **ë…¸ì¦ ë§‰í˜ (Clogging)**: í•„ë¼ë©˜íŠ¸ ì••ì¶œ ë¶ˆëŸ‰
- **ì„œí¬íŠ¸ ë¶•ê´´ (Support Failure)**: ì„œí¬íŠ¸ êµ¬ì¡° ë¶•ê´´
- **ì²« ë ˆì´ì–´ ì‹¤íŒ¨ (First Layer Failure)**: ë² ë“œ ì ‘ì°© ì‹¤íŒ¨

### í•µì‹¬ ê¸°ëŠ¥
1. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: WebRTC ìŠ¤íŠ¸ë¦¼ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ ë° ë¶„ì„
2. **AI ë¶ˆëŸ‰ ê°ì§€**: YOLO ë˜ëŠ” Foundation Model ê¸°ë°˜ ì‹¤ì‹œê°„ ì¶”ë¡ 
3. **ì˜ˆì¸¡ ì •ë³´ ìƒì„±**: ë¶ˆëŸ‰ íƒ€ì…, ì‹ ë¢°ë„, ìœ„ì¹˜, ì‹œê°„ ë“±
4. **MQTT ì•Œë¦¼**: ë¶ˆëŸ‰ ê°ì§€ ì‹œ ì‹¤ì‹œê°„ ì•Œë¦¼
5. **ì´ë ¥ ê´€ë¦¬**: DBì— ê°ì§€ ì´ë ¥ ì €ì¥ ë° ëŒ€ì‹œë³´ë“œ ì œê³µ

---

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Frontend (Web)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ CameraFeed   â”‚  â”‚ AI Dashboard â”‚  â”‚ Alert Panel  â”‚      â”‚
â”‚  â”‚ (WebRTC)     â”‚  â”‚ (Detection)  â”‚  â”‚ (MQTT Sub)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                  â”‚                  â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â”‚                  â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â”‚                  â”‚ HTTP API         â”‚ MQTT          â”‚
â”‚         â–¼                  â–¼                  â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚           FastAPI Server (main.py)               â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚  â”‚  â”‚ WebRTC     â”‚  â”‚ AI Model   â”‚  â”‚ MQTT       â”‚ â”‚        â”‚
â”‚  â”‚  â”‚ Frame      â”‚  â”‚ Inference  â”‚  â”‚ Publisher  â”‚ â”‚        â”‚
â”‚  â”‚  â”‚ Capture    â”‚  â”‚ Service    â”‚  â”‚            â”‚ â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚               â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚     AI Inference Engine                â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚
â”‚  â”‚  â”‚ YOLOv8   â”‚  â”‚ SAM2 / Florence2 â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚ Detectionâ”‚  â”‚ Segmentation     â”‚  â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Supabase DB      â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚  â”‚ detections   â”‚  â”‚  - ê°ì§€ ì´ë ¥
   â”‚  â”‚ print_jobs   â”‚  â”‚  - ì¶œë ¥ ì‘ì—…
   â”‚  â”‚ cameras      â”‚  â”‚  - ì¹´ë©”ë¼ ì •ë³´
   â”‚  â”‚ alerts       â”‚  â”‚  - ì•Œë¦¼ ì´ë ¥
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ê¸°ìˆ  ìŠ¤íƒ

### Backend (Python)
- **FastAPI**: REST API ì„œë²„
- **OpenCV**: ë¹„ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬
- **PyTorch**: ë”¥ëŸ¬ë‹ ì¶”ë¡ 
- **Ultralytics YOLOv8**: ê°ì²´ ê°ì§€
- **Transformers (Hugging Face)**: Foundation Models
  - SAM2 (Segment Anything Model 2)
  - Florence-2 (Vision-Language Model)
  - CLIP (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­)
- **paho-mqtt**: MQTT í´ë¼ì´ì–¸íŠ¸
- **aiortc**: WebRTC ì²˜ë¦¬ (ì„ íƒì‚¬í•­)

### Frontend (React/TypeScript)
- **WebRTC API**: ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë°
- **MQTT.js**: ì‹¤ì‹œê°„ ì•Œë¦¼ ìˆ˜ì‹ 
- **Canvas API**: ê°ì§€ ê²°ê³¼ ì˜¤ë²„ë ˆì´

### Database & Storage
- **Supabase PostgreSQL**: ë©”íƒ€ë°ì´í„° ì €ì¥
- **Supabase Storage**: ì´ë¯¸ì§€/ì˜ìƒ ì €ì¥
- **Redis** (ì„ íƒ): ì‹¤ì‹œê°„ ìƒíƒœ ìºì‹±

### AI Models
| ëª¨ë¸ | ìš©ë„ | í¬ê¸° | ì¶”ë¡  ì†ë„ |
|------|------|------|-----------|
| YOLOv8n | ê²½ëŸ‰ ê°ì§€ (ìŠ¤íŒŒê²Œí‹°, ì™€í•‘) | ~6MB | ~200 FPS |
| YOLOv8s | ì¤‘ê°„ ì •í™•ë„ | ~22MB | ~120 FPS |
| YOLOv8m | ë†’ì€ ì •í™•ë„ | ~50MB | ~45 FPS |
| SAM2-tiny | ì„¸ê·¸ë©˜í…Œì´ì…˜ | ~40MB | ~30 FPS |
| Florence-2 | ë¹„ì „-ì–¸ì–´ | ~230MB | ~10 FPS |

**ê¶Œì¥**: ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•´ **YOLOv8n + SAM2-tiny** ì¡°í•©

---

## ê°œë°œ ë‹¨ê³„ë³„ ê³„íš

### Phase 1: ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶• (Week 1-2)

#### 1.1 WebRTC í”„ë ˆì„ ì¶”ì¶œ API
**íŒŒì¼**: `webrtc_capture.py`

```python
"""
WebRTC ìŠ¤íŠ¸ë¦¼ì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ AI ë¶„ì„ìš©ìœ¼ë¡œ ì œê³µ
"""
import cv2
import numpy as np
from typing import Optional, Tuple
import asyncio
import aiohttp

class WebRTCFrameCapture:
    def __init__(self, stream_url: str):
        """
        Args:
            stream_url: WebRTC ìŠ¤íŠ¸ë¦¼ URL
        """
        self.stream_url = stream_url
        self.cap = None
        self.last_frame = None
        self.frame_interval = 1.0  # 1ì´ˆë§ˆë‹¤ 1í”„ë ˆì„ ì¶”ì¶œ (ê¸°ë³¸ê°’)

    async def start_capture(self):
        """ìŠ¤íŠ¸ë¦¼ ìº¡ì²˜ ì‹œì‘"""
        # WebRTC -> HTTP í”„ë¡ì‹œ ì‚¬ìš© ë˜ëŠ” ì§ì ‘ ì—°ê²°
        # go2rtc, mediamtx ë“±ì˜ í”„ë¡ì‹œ í™œìš©
        pass

    async def get_frame(self) -> Optional[np.ndarray]:
        """í˜„ì¬ í”„ë ˆì„ ë°˜í™˜"""
        pass

    def stop_capture(self):
        """ìº¡ì²˜ ì¤‘ì§€"""
        pass
```

**ì‘ì—…**:
- [ ] WebRTC ìŠ¤íŠ¸ë¦¼ â†’ OpenCV ë³€í™˜ ë¡œì§ êµ¬í˜„
- [ ] í”„ë ˆì„ ì¶”ì¶œ ì£¼ê¸° ì„¤ì • (FPS ì¡°ì ˆ)
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì—°ê²° ë¡œì§
- [ ] ë©”ëª¨ë¦¬ ê´€ë¦¬ (í”„ë ˆì„ ë²„í¼ í¬ê¸° ì œí•œ)

#### 1.2 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„

**Supabase í…Œì´ë¸” ìƒì„±**:

```sql
-- ê°ì§€ ì´ë ¥ í…Œì´ë¸”
CREATE TABLE print_detections (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES auth.users(id),
    device_uuid UUID NOT NULL,
    print_job_id UUID REFERENCES print_jobs(id),

    -- ê°ì§€ ì •ë³´
    detection_type VARCHAR(50) NOT NULL, -- 'spaghetti', 'warping', 'layer_separation', etc.
    confidence FLOAT NOT NULL,           -- 0.0 ~ 1.0
    severity VARCHAR(20) NOT NULL,       -- 'low', 'medium', 'high', 'critical'

    -- ìœ„ì¹˜ ì •ë³´ (bounding box)
    bbox_x INTEGER,
    bbox_y INTEGER,
    bbox_width INTEGER,
    bbox_height INTEGER,

    -- ì´ë¯¸ì§€ ì €ì¥
    frame_image_url TEXT,                -- Supabase Storage URL
    annotated_image_url TEXT,            -- ë°”ìš´ë”© ë°•ìŠ¤ í‘œì‹œëœ ì´ë¯¸ì§€

    -- ë©”íƒ€ë°ì´í„°
    layer_number INTEGER,                -- í˜„ì¬ ë ˆì´ì–´
    print_progress FLOAT,                -- ì¶œë ¥ ì§„í–‰ë¥  (%)
    timestamp TIMESTAMPTZ DEFAULT NOW(),

    -- ì¸ë±ìŠ¤
    CONSTRAINT detections_device_time_idx
        FOREIGN KEY (device_uuid, timestamp)
);

-- ì•Œë¦¼ ì„¤ì • í…Œì´ë¸”
CREATE TABLE alert_settings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES auth.users(id),
    device_uuid UUID,

    -- ì•Œë¦¼ ì¡°ê±´
    enabled BOOLEAN DEFAULT TRUE,
    detection_types TEXT[],              -- ['spaghetti', 'warping', ...]
    min_confidence FLOAT DEFAULT 0.7,
    severity_threshold VARCHAR(20) DEFAULT 'medium',

    -- ì•Œë¦¼ ì±„ë„
    mqtt_enabled BOOLEAN DEFAULT TRUE,
    email_enabled BOOLEAN DEFAULT FALSE,
    sms_enabled BOOLEAN DEFAULT FALSE,

    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ì¶œë ¥ ì‘ì—… í…Œì´ë¸” (ê¸°ì¡´ì— ì—†ë‹¤ë©´)
CREATE TABLE print_jobs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES auth.users(id),
    device_uuid UUID NOT NULL,

    gcode_file_id UUID,
    model_name VARCHAR(255),

    status VARCHAR(50) DEFAULT 'printing', -- 'printing', 'paused', 'completed', 'failed'
    progress FLOAT DEFAULT 0.0,
    current_layer INTEGER DEFAULT 0,
    total_layers INTEGER,

    -- AI ëª¨ë‹ˆí„°ë§ ìƒíƒœ
    ai_monitoring_enabled BOOLEAN DEFAULT TRUE,
    detection_count INTEGER DEFAULT 0,

    started_at TIMESTAMPTZ DEFAULT NOW(),
    completed_at TIMESTAMPTZ,

    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_detections_user_time ON print_detections(user_id, timestamp DESC);
CREATE INDEX idx_detections_device_time ON print_detections(device_uuid, timestamp DESC);
CREATE INDEX idx_detections_job ON print_detections(print_job_id);
CREATE INDEX idx_print_jobs_user ON print_jobs(user_id, started_at DESC);
```

**ì‘ì—…**:
- [ ] Supabase SQL ì—ë””í„°ì—ì„œ í…Œì´ë¸” ìƒì„±
- [ ] RLS (Row Level Security) ì •ì±… ì„¤ì •
- [ ] API ê¶Œí•œ í™•ì¸

---

### Phase 2: AI ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ (Week 3-4)

#### 2.1 ëª¨ë¸ ì„ íƒ ì „ëµ

**Option 1: YOLOv8 Custom Training** (ê¶Œì¥)
- **ì¥ì **: ë¹ ë¥¸ ì¶”ë¡  ì†ë„, ì»¤ìŠ¤í…€ ë°ì´í„° í•™ìŠµ ê°€ëŠ¥
- **ë‹¨ì **: ë¼ë²¨ë§ ë°ì´í„° í•„ìš”

**Option 2: Foundation Model (Florence-2, SAM2)**
- **ì¥ì **: Zero-shot ë˜ëŠ” Few-shot í•™ìŠµ, ë¼ë²¨ë§ ìµœì†Œí™”
- **ë‹¨ì **: ëŠë¦° ì¶”ë¡  ì†ë„, GPU í•„ìˆ˜

**Option 3: Hybrid Approach** (ìµœì )
- YOLOv8ìœ¼ë¡œ 1ì°¨ ê°ì§€ (ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹)
- Florence-2ë¡œ 2ì°¨ ê²€ì¦ (ì •í™•ë„ í–¥ìƒ)

#### 2.2 ë°ì´í„° ìˆ˜ì§‘

**í•„ìš” ë°ì´í„°**:
- ì •ìƒ ì¶œë ¥ ì˜ìƒ: 500+ í”„ë ˆì„
- ìŠ¤íŒŒê²Œí‹°í™”: 200+ í”„ë ˆì„
- ì™€í•‘: 150+ í”„ë ˆì„
- ë ˆì´ì–´ ë¶„ë¦¬: 100+ í”„ë ˆì„
- ë…¸ì¦ ë§‰í˜: 100+ í”„ë ˆì„
- ì„œí¬íŠ¸ ë¶•ê´´: 100+ í”„ë ˆì„

**ë°ì´í„° ì†ŒìŠ¤**:
1. **ìì²´ ìˆ˜ì§‘**: ì‹¤ì œ í”„ë¦°í„°ì—ì„œ ì˜ë„ì  ë¶ˆëŸ‰ ìœ ë°œ
2. **ê³µê°œ ë°ì´í„°ì…‹**:
   - [Spaghetti Detective Dataset](https://github.com/TheSpaghettiDetective/ml_api)
   - [3D Print Monitor Dataset (Kaggle)](https://www.kaggle.com/datasets)
3. **í•©ì„± ë°ì´í„°**: Blenderë¡œ ì‹œë®¬ë ˆì´ì…˜

#### 2.3 YOLOv8 í•™ìŠµ íŒŒì´í”„ë¼ì¸

**íŒŒì¼**: `train_yolo_detector.py`

```python
"""
YOLOv8 ì»¤ìŠ¤í…€ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""
from ultralytics import YOLO
import yaml

# ë°ì´í„°ì…‹ êµ¬ì¡°
# dataset/
#   â”œâ”€â”€ images/
#   â”‚   â”œâ”€â”€ train/
#   â”‚   â””â”€â”€ val/
#   â””â”€â”€ labels/
#       â”œâ”€â”€ train/
#       â””â”€â”€ val/

# dataset.yaml ìƒì„±
dataset_config = {
    'path': './dataset',
    'train': 'images/train',
    'val': 'images/val',
    'names': {
        0: 'spaghetti',
        1: 'warping',
        2: 'layer_separation',
        3: 'clogging',
        4: 'support_failure',
        5: 'first_layer_fail'
    }
}

with open('dataset.yaml', 'w') as f:
    yaml.dump(dataset_config, f)

# ëª¨ë¸ í•™ìŠµ
model = YOLO('yolov8n.pt')  # nano ëª¨ë¸ (ê°€ì¥ ë¹ ë¦„)

results = model.train(
    data='dataset.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    device=0,  # GPU 0
    project='print_detector',
    name='yolov8n_v1',

    # Augmentation
    hsv_h=0.015,
    hsv_s=0.7,
    hsv_v=0.4,
    degrees=10,
    translate=0.1,
    scale=0.5,
    shear=0.0,
    perspective=0.0,
    flipud=0.5,
    fliplr=0.5,
    mosaic=1.0,
    mixup=0.0,
)

# ëª¨ë¸ ê²€ì¦
metrics = model.val()
print(f"mAP50: {metrics.box.map50}")
print(f"mAP50-95: {metrics.box.map}")

# ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
model.export(format='onnx')  # ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜ (ë” ë¹ ë¥¸ ì¶”ë¡ )
```

**ì‘ì—…**:
- [ ] ë°ì´í„° ìˆ˜ì§‘ ë° ë¼ë²¨ë§ (Roboflow, LabelImg ì‚¬ìš©)
- [ ] í•™ìŠµ ë°ì´í„° ì¦ê°• (Augmentation)
- [ ] ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ìµœì¢… ëª¨ë¸ ì„ ì • (ì •í™•ë„ vs ì†ë„)

#### 2.4 Foundation Model í†µí•© (ì„ íƒ)

**Florence-2 ì˜ˆì œ**:
```python
from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image

processor = AutoProcessor.from_pretrained("microsoft/Florence-2-base")
model = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-base")

def detect_with_florence(image_path: str, prompt: str = "<OD>"):
    """
    Florence-2ë¡œ ê°ì²´ ê°ì§€

    Args:
        image_path: ì´ë¯¸ì§€ ê²½ë¡œ
        prompt: "<OD>" (Object Detection), "<CAPTION>" (Captioning)
    """
    image = Image.open(image_path)

    inputs = processor(text=prompt, images=image, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=1024)

    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return result
```

---

### Phase 3: AI ì¶”ë¡  ì„œë¹„ìŠ¤ ê°œë°œ (Week 5-6)

#### 3.1 AI ì¶”ë¡  ëª¨ë“ˆ

**íŒŒì¼**: `ai_inference.py`

```python
"""
ì‹¤ì‹œê°„ AI ì¶”ë¡  ì„œë¹„ìŠ¤
"""
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from typing import List, Dict, Optional, Tuple
import logging
from datetime import datetime
from pathlib import Path

logger = logging.getLogger("uvicorn.error")

# ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
YOLO_MODEL = None
YOLO_MODEL_PATH = "./models/yolov8n_print_detector.pt"
CONFIDENCE_THRESHOLD = 0.5
IOU_THRESHOLD = 0.45

# ë¶ˆëŸ‰ íƒ€ì…ë³„ ì‹¬ê°ë„ ë§¤í•‘
SEVERITY_MAP = {
    'spaghetti': 'critical',
    'warping': 'high',
    'layer_separation': 'high',
    'clogging': 'medium',
    'support_failure': 'medium',
    'first_layer_fail': 'critical'
}

class AIInferenceService:
    """AI ì¶”ë¡  ì„œë¹„ìŠ¤"""

    def __init__(self, model_path: str = YOLO_MODEL_PATH):
        """
        Args:
            model_path: YOLO ëª¨ë¸ ê²½ë¡œ
        """
        self.model = self._load_model(model_path)
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"[AI] Model loaded on {self.device}")

    def _load_model(self, model_path: str) -> YOLO:
        """ëª¨ë¸ ë¡œë“œ"""
        if not Path(model_path).exists():
            logger.error(f"[AI] Model not found: {model_path}")
            raise FileNotFoundError(f"Model not found: {model_path}")

        model = YOLO(model_path)
        return model

    async def predict(
        self,
        frame: np.ndarray,
        conf_threshold: float = CONFIDENCE_THRESHOLD,
        iou_threshold: float = IOU_THRESHOLD
    ) -> List[Dict]:
        """
        í”„ë ˆì„ì—ì„œ ë¶ˆëŸ‰ ê°ì§€

        Args:
            frame: OpenCV ì´ë¯¸ì§€ (BGR)
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            iou_threshold: IoU ì„ê³„ê°’

        Returns:
            ê°ì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            [
                {
                    'detection_type': 'spaghetti',
                    'confidence': 0.95,
                    'bbox': [x, y, w, h],
                    'severity': 'critical',
                    'timestamp': '2025-01-26T10:30:00Z'
                },
                ...
            ]
        """
        if frame is None or frame.size == 0:
            logger.warning("[AI] Empty frame received")
            return []

        try:
            # YOLO ì¶”ë¡ 
            results = self.model.predict(
                frame,
                conf=conf_threshold,
                iou=iou_threshold,
                verbose=False,
                device=self.device
            )

            detections = []

            for result in results:
                boxes = result.boxes

                for i, box in enumerate(boxes):
                    cls_id = int(box.cls[0])
                    conf = float(box.conf[0])
                    xyxy = box.xyxy[0].cpu().numpy()

                    # í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                    class_name = self.model.names[cls_id]

                    # ë°”ìš´ë”© ë°•ìŠ¤ [x, y, w, h] í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    x1, y1, x2, y2 = xyxy
                    bbox = [
                        int(x1),
                        int(y1),
                        int(x2 - x1),
                        int(y2 - y1)
                    ]

                    detection = {
                        'detection_type': class_name,
                        'confidence': round(conf, 4),
                        'bbox': bbox,
                        'severity': SEVERITY_MAP.get(class_name, 'low'),
                        'timestamp': datetime.utcnow().isoformat() + 'Z'
                    }

                    detections.append(detection)

                    logger.info(
                        f"[AI] Detected {class_name} "
                        f"(conf={conf:.2f}, severity={detection['severity']})"
                    )

            return detections

        except Exception as e:
            logger.error(f"[AI] Prediction failed: {str(e)}")
            return []

    def draw_detections(
        self,
        frame: np.ndarray,
        detections: List[Dict]
    ) -> np.ndarray:
        """
        í”„ë ˆì„ì— ê°ì§€ ê²°ê³¼ ê·¸ë¦¬ê¸°

        Args:
            frame: ì›ë³¸ í”„ë ˆì„
            detections: ê°ì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì–´ë…¸í…Œì´ì…˜ì´ ê·¸ë ¤ì§„ í”„ë ˆì„
        """
        annotated = frame.copy()

        # ì‹¬ê°ë„ë³„ ìƒ‰ìƒ
        color_map = {
            'critical': (0, 0, 255),    # ë¹¨ê°•
            'high': (0, 165, 255),      # ì£¼í™©
            'medium': (0, 255, 255),    # ë…¸ë‘
            'low': (0, 255, 0)          # ì´ˆë¡
        }

        for det in detections:
            x, y, w, h = det['bbox']
            severity = det['severity']
            conf = det['confidence']
            det_type = det['detection_type']

            color = color_map.get(severity, (255, 255, 255))

            # ë°”ìš´ë”© ë°•ìŠ¤
            cv2.rectangle(annotated, (x, y), (x+w, y+h), color, 2)

            # ë ˆì´ë¸”
            label = f"{det_type}: {conf:.2f}"
            label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            cv2.rectangle(
                annotated,
                (x, y - label_size[1] - 10),
                (x + label_size[0], y),
                color,
                -1
            )
            cv2.putText(
                annotated,
                label,
                (x, y - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 0, 0),
                1
            )

        return annotated

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_ai_service: Optional[AIInferenceService] = None

def get_ai_service() -> AIInferenceService:
    """AI ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ai_service
    if _ai_service is None:
        _ai_service = AIInferenceService()
    return _ai_service
```

**ì‘ì—…**:
- [ ] YOLO ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  êµ¬í˜„
- [ ] ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° ê¸°ëŠ¥
- [ ] ì„±ëŠ¥ ìµœì í™” (ë°°ì¹˜ ì²˜ë¦¬, GPU í™œìš©)
- [ ] ì—ëŸ¬ í•¸ë“¤ë§

#### 3.2 ëª¨ë‹ˆí„°ë§ API ì—”ë“œí¬ì¸íŠ¸

**íŒŒì¼**: `main.py` ì¶”ê°€

```python
"""
AI ëª¨ë‹ˆí„°ë§ API ì—”ë“œí¬ì¸íŠ¸
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional
import cv2
import numpy as np
from datetime import datetime
import uuid

from ai_inference import get_ai_service
from supabase_db import save_detection, get_print_job
from supabase_storage import upload_frame_image
from mqtt_notification import send_detection_alert

router = APIRouter(prefix="/v1/ai-monitoring", tags=["AI Monitoring"])

class StartMonitoringRequest(BaseModel):
    device_uuid: str
    print_job_id: Optional[str] = None
    user_id: str
    confidence_threshold: float = 0.5
    frame_interval: int = 5  # 5ì´ˆë§ˆë‹¤ ë¶„ì„

class DetectionResponse(BaseModel):
    detection_id: str
    detection_type: str
    confidence: float
    severity: str
    bbox: List[int]
    frame_url: Optional[str]
    timestamp: str

@router.post("/start")
async def start_monitoring(request: StartMonitoringRequest):
    """
    AI ëª¨ë‹ˆí„°ë§ ì‹œì‘

    - WebRTC ìŠ¤íŠ¸ë¦¼ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ ì‹œì‘
    - ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ AI ë¶„ì„
    """
    # TODO: WebRTC ìº¡ì²˜ ì‹œì‘
    # TODO: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë“±ë¡

    return {
        "status": "ok",
        "message": f"Monitoring started for device {request.device_uuid}",
        "monitoring_id": str(uuid.uuid4())
    }

@router.post("/stop")
async def stop_monitoring(device_uuid: str):
    """AI ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
    # TODO: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì·¨ì†Œ

    return {
        "status": "ok",
        "message": f"Monitoring stopped for device {device_uuid}"
    }

@router.post("/analyze-frame", response_model=List[DetectionResponse])
async def analyze_frame(
    device_uuid: str,
    user_id: str,
    file: UploadFile = File(...),
    print_job_id: Optional[str] = None,
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """
    ë‹¨ì¼ í”„ë ˆì„ ë¶„ì„

    - ì´ë¯¸ì§€ ì—…ë¡œë“œí•˜ì—¬ ì¦‰ì‹œ AI ë¶„ì„
    - ê°ì§€ ê²°ê³¼ DB ì €ì¥ ë° MQTT ì•Œë¦¼
    """
    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        contents = await file.read()
        nparr = np.frombuffer(contents, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if frame is None:
            raise HTTPException(status_code=400, detail="Invalid image")

        # AI ì¶”ë¡ 
        ai_service = get_ai_service()
        detections = await ai_service.predict(frame, conf_threshold=0.5)

        if not detections:
            return []

        # ì–´ë…¸í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ìƒì„±
        annotated_frame = ai_service.draw_detections(frame, detections)

        # DB ì €ì¥ ë° ì•Œë¦¼ (ë°±ê·¸ë¼ìš´ë“œ)
        for det in detections:
            detection_id = str(uuid.uuid4())
            det['detection_id'] = detection_id

            # ì´ë¯¸ì§€ ì—…ë¡œë“œ
            frame_url = await upload_frame_image(
                user_id,
                device_uuid,
                detection_id,
                annotated_frame
            )
            det['frame_url'] = frame_url

            # DB ì €ì¥
            background_tasks.add_task(
                save_detection,
                user_id=user_id,
                device_uuid=device_uuid,
                print_job_id=print_job_id,
                detection=det
            )

            # MQTT ì•Œë¦¼
            background_tasks.add_task(
                send_detection_alert,
                user_id=user_id,
                device_uuid=device_uuid,
                detection=det
            )

        return [DetectionResponse(**det) for det in detections]

    except Exception as e:
        logger.error(f"[AI] Frame analysis failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/detections/history")
async def get_detection_history(
    user_id: str,
    device_uuid: Optional[str] = None,
    print_job_id: Optional[str] = None,
    limit: int = 50
):
    """ê°ì§€ ì´ë ¥ ì¡°íšŒ"""
    # TODO: DBì—ì„œ ê°ì§€ ì´ë ¥ ì¡°íšŒ
    pass

@router.get("/detections/stats")
async def get_detection_stats(
    user_id: str,
    device_uuid: Optional[str] = None,
    time_range: str = "24h"  # 1h, 24h, 7d, 30d
):
    """
    ê°ì§€ í†µê³„

    - ë¶ˆëŸ‰ íƒ€ì…ë³„ ë°œìƒ ë¹ˆë„
    - ì‹œê°„ëŒ€ë³„ ì¶”ì´
    - í‰ê·  ì‹ ë¢°ë„
    """
    # TODO: í†µê³„ ì¿¼ë¦¬
    pass
```

**ì‘ì—…**:
- [ ] ëª¨ë‹ˆí„°ë§ ì‹œì‘/ì¤‘ì§€ API êµ¬í˜„
- [ ] í”„ë ˆì„ ë¶„ì„ API êµ¬í˜„
- [ ] ì´ë ¥ ì¡°íšŒ API êµ¬í˜„
- [ ] í†µê³„ API êµ¬í˜„

---

### Phase 4: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì›Œì»¤ (Week 7)

#### 4.1 ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤

**íŒŒì¼**: `monitoring_worker.py`

```python
"""
ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ì›Œì»¤
WebRTC ìŠ¤íŠ¸ë¦¼ì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œí•˜ì—¬ AI ë¶„ì„
"""
import asyncio
from typing import Dict, Optional
import logging
from datetime import datetime

from webrtc_capture import WebRTCFrameCapture
from ai_inference import get_ai_service
from supabase_db import save_detection, update_print_job_status
from supabase_storage import upload_frame_image
from mqtt_notification import send_detection_alert

logger = logging.getLogger("uvicorn.error")

class MonitoringWorker:
    """ëª¨ë‹ˆí„°ë§ ì›Œì»¤"""

    def __init__(
        self,
        device_uuid: str,
        user_id: str,
        stream_url: str,
        print_job_id: Optional[str] = None,
        frame_interval: int = 5,  # 5ì´ˆë§ˆë‹¤ ë¶„ì„
        confidence_threshold: float = 0.5
    ):
        self.device_uuid = device_uuid
        self.user_id = user_id
        self.stream_url = stream_url
        self.print_job_id = print_job_id
        self.frame_interval = frame_interval
        self.confidence_threshold = confidence_threshold

        self.capture = WebRTCFrameCapture(stream_url)
        self.ai_service = get_ai_service()
        self.is_running = False
        self.task: Optional[asyncio.Task] = None

    async def start(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_running:
            logger.warning(f"[Worker] Already running for {self.device_uuid}")
            return

        self.is_running = True
        await self.capture.start_capture()

        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
        self.task = asyncio.create_task(self._monitoring_loop())

        logger.info(f"[Worker] Started for device {self.device_uuid}")

    async def stop(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_running = False

        if self.task:
            self.task.cancel()
            try:
                await self.task
            except asyncio.CancelledError:
                pass

        self.capture.stop_capture()

        logger.info(f"[Worker] Stopped for device {self.device_uuid}")

    async def _monitoring_loop(self):
        """ë©”ì¸ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        consecutive_failures = 0
        max_failures = 5

        while self.is_running:
            try:
                # í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                frame = await self.capture.get_frame()

                if frame is None:
                    consecutive_failures += 1
                    logger.warning(
                        f"[Worker] No frame from {self.device_uuid} "
                        f"({consecutive_failures}/{max_failures})"
                    )

                    if consecutive_failures >= max_failures:
                        logger.error(
                            f"[Worker] Too many failures, stopping {self.device_uuid}"
                        )
                        await self.stop()
                        break

                    await asyncio.sleep(self.frame_interval)
                    continue

                # ì„±ê³µ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
                consecutive_failures = 0

                # AI ë¶„ì„
                detections = await self.ai_service.predict(
                    frame,
                    conf_threshold=self.confidence_threshold
                )

                if detections:
                    logger.info(
                        f"[Worker] {len(detections)} detection(s) "
                        f"for {self.device_uuid}"
                    )

                    # ì–´ë…¸í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ìƒì„±
                    annotated_frame = self.ai_service.draw_detections(
                        frame,
                        detections
                    )

                    # ê° ê°ì§€ ê²°ê³¼ ì²˜ë¦¬
                    for det in detections:
                        await self._process_detection(det, annotated_frame)

                # ëŒ€ê¸°
                await asyncio.sleep(self.frame_interval)

            except asyncio.CancelledError:
                logger.info(f"[Worker] Task cancelled for {self.device_uuid}")
                break

            except Exception as e:
                logger.error(f"[Worker] Error in monitoring loop: {str(e)}")
                consecutive_failures += 1

                if consecutive_failures >= max_failures:
                    logger.error(f"[Worker] Too many errors, stopping")
                    await self.stop()
                    break

                await asyncio.sleep(self.frame_interval)

    async def _process_detection(self, detection: Dict, frame):
        """ê°ì§€ ê²°ê³¼ ì²˜ë¦¬"""
        import uuid

        detection_id = str(uuid.uuid4())
        detection['detection_id'] = detection_id

        try:
            # ì´ë¯¸ì§€ Supabase Storage ì—…ë¡œë“œ
            frame_url = await upload_frame_image(
                self.user_id,
                self.device_uuid,
                detection_id,
                frame
            )
            detection['frame_url'] = frame_url

            # DB ì €ì¥
            await save_detection(
                user_id=self.user_id,
                device_uuid=self.device_uuid,
                print_job_id=self.print_job_id,
                detection=detection
            )

            # MQTT ì•Œë¦¼
            await send_detection_alert(
                user_id=self.user_id,
                device_uuid=self.device_uuid,
                detection=detection
            )

            # Critical ê°ì§€ ì‹œ ì¶œë ¥ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            if detection['severity'] == 'critical' and self.print_job_id:
                await update_print_job_status(
                    self.print_job_id,
                    status='paused',
                    reason=f"Critical detection: {detection['detection_type']}"
                )

        except Exception as e:
            logger.error(f"[Worker] Failed to process detection: {str(e)}")

# ì›Œì»¤ ê´€ë¦¬ì
class MonitoringManager:
    """ëª¨ë‹ˆí„°ë§ ì›Œì»¤ ê´€ë¦¬ì (ì‹±ê¸€í†¤)"""

    def __init__(self):
        self.workers: Dict[str, MonitoringWorker] = {}

    async def start_monitoring(
        self,
        device_uuid: str,
        user_id: str,
        stream_url: str,
        **kwargs
    ):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if device_uuid in self.workers:
            logger.warning(f"[Manager] Already monitoring {device_uuid}")
            return

        worker = MonitoringWorker(
            device_uuid=device_uuid,
            user_id=user_id,
            stream_url=stream_url,
            **kwargs
        )

        await worker.start()
        self.workers[device_uuid] = worker

        logger.info(f"[Manager] Monitoring started for {device_uuid}")

    async def stop_monitoring(self, device_uuid: str):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if device_uuid not in self.workers:
            logger.warning(f"[Manager] Not monitoring {device_uuid}")
            return

        worker = self.workers[device_uuid]
        await worker.stop()
        del self.workers[device_uuid]

        logger.info(f"[Manager] Monitoring stopped for {device_uuid}")

    async def stop_all(self):
        """ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        for device_uuid in list(self.workers.keys()):
            await self.stop_monitoring(device_uuid)

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_monitoring_manager: Optional[MonitoringManager] = None

def get_monitoring_manager() -> MonitoringManager:
    """ëª¨ë‹ˆí„°ë§ ë§¤ë‹ˆì € ì‹±ê¸€í†¤"""
    global _monitoring_manager
    if _monitoring_manager is None:
        _monitoring_manager = MonitoringManager()
    return _monitoring_manager
```

**ì‘ì—…**:
- [ ] ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ êµ¬í˜„
- [ ] ì›Œì»¤ ê´€ë¦¬ì êµ¬í˜„
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì—°ê²° ë¡œì§
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (CPU, GPU, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰)

---

### Phase 5: í”„ë¡ íŠ¸ì—”ë“œ í†µí•© (Week 8)

#### 5.1 AI ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

**íŒŒì¼**: `packages/web/src/components/AIMonitoringPanel.tsx`

```typescript
/**
 * AI ëª¨ë‹ˆí„°ë§ íŒ¨ë„
 * - ì‹¤ì‹œê°„ ê°ì§€ ê²°ê³¼ í‘œì‹œ
 * - MQTTë¡œ ì•Œë¦¼ ìˆ˜ì‹ 
 * - ê°ì§€ ì´ë ¥ ì°¨íŠ¸
 */
import { useState, useEffect } from 'react';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { AlertCircle, CheckCircle, XCircle } from 'lucide-react';
import { onDetectionMessage } from '@shared/services/mqttService';

interface Detection {
  detection_id: string;
  detection_type: string;
  confidence: number;
  severity: 'low' | 'medium' | 'high' | 'critical';
  bbox: [number, number, number, number];
  frame_url?: string;
  timestamp: string;
}

export const AIMonitoringPanel = ({ deviceUuid }: { deviceUuid: string }) => {
  const [detections, setDetections] = useState<Detection[]>([]);
  const [isMonitoring, setIsMonitoring] = useState(false);

  // MQTT êµ¬ë…
  useEffect(() => {
    if (!deviceUuid) return;

    const unsubscribe = onDetectionMessage(deviceUuid, (payload) => {
      const detection = payload as Detection;
      setDetections(prev => [detection, ...prev].slice(0, 10)); // ìµœê·¼ 10ê°œë§Œ
    });

    return () => unsubscribe();
  }, [deviceUuid]);

  // ëª¨ë‹ˆí„°ë§ ì‹œì‘/ì¤‘ì§€
  const toggleMonitoring = async () => {
    if (isMonitoring) {
      // ì¤‘ì§€ API í˜¸ì¶œ
      await fetch(`/v1/ai-monitoring/stop?device_uuid=${deviceUuid}`, {
        method: 'POST'
      });
      setIsMonitoring(false);
    } else {
      // ì‹œì‘ API í˜¸ì¶œ
      await fetch('/v1/ai-monitoring/start', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          device_uuid: deviceUuid,
          user_id: 'user-id', // ì‹¤ì œ user_id
          confidence_threshold: 0.5,
          frame_interval: 5
        })
      });
      setIsMonitoring(true);
    }
  };

  // ì‹¬ê°ë„ë³„ ìƒ‰ìƒ
  const getSeverityColor = (severity: string) => {
    switch (severity) {
      case 'critical': return 'bg-red-500';
      case 'high': return 'bg-orange-500';
      case 'medium': return 'bg-yellow-500';
      case 'low': return 'bg-green-500';
      default: return 'bg-gray-500';
    }
  };

  return (
    <Card>
      <CardHeader>
        <CardTitle className="flex items-center justify-between">
          <span>AI Monitoring</span>
          <button onClick={toggleMonitoring}>
            {isMonitoring ? 'Stop' : 'Start'}
          </button>
        </CardTitle>
      </CardHeader>
      <CardContent>
        {/* ì‹¤ì‹œê°„ ê°ì§€ ëª©ë¡ */}
        <div className="space-y-2">
          {detections.map(det => (
            <div key={det.detection_id} className="flex items-center gap-2 p-2 border rounded">
              <Badge className={getSeverityColor(det.severity)}>
                {det.severity}
              </Badge>
              <span className="font-medium">{det.detection_type}</span>
              <span className="text-sm text-gray-500">
                {(det.confidence * 100).toFixed(1)}%
              </span>
              {det.frame_url && (
                <a href={det.frame_url} target="_blank" className="text-blue-500">
                  View
                </a>
              )}
            </div>
          ))}
        </div>
      </CardContent>
    </Card>
  );
};
```

**ì‘ì—…**:
- [ ] AI ëª¨ë‹ˆí„°ë§ íŒ¨ë„ ì»´í¬ë„ŒíŠ¸
- [ ] MQTT ê°ì§€ ì•Œë¦¼ ìˆ˜ì‹ 
- [ ] ê°ì§€ ì´ë ¥ ì°¨íŠ¸ (Recharts)
- [ ] ì´ë¯¸ì§€ ì˜¤ë²„ë ˆì´ (Canvas)

#### 5.2 MQTT í† í”½ ì¶”ê°€

**íŒŒì¼**: `mqtt_notification.py` ìˆ˜ì •

```python
# ìƒˆë¡œìš´ í† í”½ ì¶”ê°€
TOPIC_AI_DETECTION = "ai/detection/{device_uuid}"

def send_detection_alert(
    user_id: str,
    device_uuid: str,
    detection: Dict
) -> bool:
    """
    AI ê°ì§€ ì•Œë¦¼ ì „ì†¡

    Topic: ai/detection/{device_uuid}
    Payload:
        {
            "detection_id": "uuid",
            "detection_type": "spaghetti",
            "confidence": 0.95,
            "severity": "critical",
            "bbox": [x, y, w, h],
            "frame_url": "https://...",
            "timestamp": "2025-01-26T10:30:00Z"
        }
    """
    try:
        client = create_mqtt_client()
        client.connect(MQTT_BROKER, MQTT_PORT, keepalive=60)
        client.loop_start()

        topic = TOPIC_AI_DETECTION.format(device_uuid=device_uuid)

        payload = {
            "detection_id": detection['detection_id'],
            "detection_type": detection['detection_type'],
            "confidence": detection['confidence'],
            "severity": detection['severity'],
            "bbox": detection['bbox'],
            "frame_url": detection.get('frame_url'),
            "timestamp": detection['timestamp']
        }

        message = json.dumps(payload)
        result = client.publish(topic, message, qos=1, retain=False)

        result.wait_for_publish(timeout=5)

        if result.is_published():
            logger.info(f"[MQTT] Detection alert sent: {device_uuid}")
            return True
        else:
            logger.error(f"[MQTT] Failed to send detection alert")
            return False

        client.loop_stop()
        client.disconnect()

        return True

    except Exception as e:
        logger.error(f"[MQTT] Error sending detection alert: {e}")
        return False
```

---

### Phase 6: í…ŒìŠ¤íŠ¸ ë° ìµœì í™” (Week 9-10)

#### 6.1 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ í•­ëª©**:
- [ ] ëª¨ë¸ ì¶”ë¡  ì†ë„ (FPS)
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- [ ] GPU í™œìš©ë¥ 
- [ ] ë™ì‹œ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥ ë””ë°”ì´ìŠ¤ ìˆ˜
- [ ] ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­

**ìµœì í™” ë°©ë²•**:
1. **ONNX ë³€í™˜**: PyTorch â†’ ONNX (2-3ë°° ë¹ ë¦„)
2. **TensorRT ìµœì í™”**: NVIDIA GPU ì „ìš©
3. **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ í”„ë ˆì„ í•œ ë²ˆì— ì²˜ë¦¬
4. **ë¹„ë™ê¸° ì²˜ë¦¬**: I/O ë¸”ë¡œí‚¹ ì œê±°

#### 6.2 ì •í™•ë„ í‰ê°€

**í‰ê°€ ì§€í‘œ**:
- Precision (ì •ë°€ë„)
- Recall (ì¬í˜„ìœ¨)
- F1 Score
- mAP (mean Average Precision)
- False Positive Rate

**í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹**:
- ì‹¤ì œ í”„ë¦°í„° ì¶œë ¥ ì˜ìƒ 100ê±´
- ì •ìƒ/ë¶ˆëŸ‰ ë¹„ìœ¨ 7:3

---

### Phase 7: ë°°í¬ ë° ìš´ì˜ (Week 11-12)

#### 7.1 Docker ì»¨í…Œì´ë„ˆí™”

**íŒŒì¼**: `Dockerfile.ai`

```dockerfile
FROM python:3.11-slim

# CUDA ì§€ì› (GPU ì‚¬ìš© ì‹œ)
# FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ëª¨ë¸ íŒŒì¼ ë³µì‚¬
COPY models/ /app/models/

# ì†ŒìŠ¤ ì½”ë“œ ë³µì‚¬
COPY *.py /app/

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 7000

# ì„œë²„ ì‹¤í–‰
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "7000"]
```

**docker-compose.yml ìˆ˜ì •**:

```yaml
services:
  ai-server:
    build:
      context: .
      dockerfile: Dockerfile.ai
    ports:
      - "7000:7000"
    volumes:
      - ./models:/app/models:ro
      - ./output:/app/output
    environment:
      - CUDA_VISIBLE_DEVICES=0  # GPU 0 ì‚¬ìš©
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

#### 7.2 ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

**Prometheus ë©”íŠ¸ë¦­ ì¶”ê°€**:

```python
from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
detection_counter = Counter(
    'ai_detections_total',
    'Total number of detections',
    ['detection_type', 'severity']
)

inference_duration = Histogram(
    'ai_inference_duration_seconds',
    'AI inference duration'
)

active_monitors = Gauge(
    'ai_active_monitors',
    'Number of active monitoring workers'
)

# ì‚¬ìš© ì˜ˆì‹œ
@inference_duration.time()
async def predict(...):
    # ...
    for det in detections:
        detection_counter.labels(
            detection_type=det['detection_type'],
            severity=det['severity']
        ).inc()
```

#### 7.3 ì•Œë¦¼ ë° ëŒ€ì‘

**ìë™ ëŒ€ì‘ ì‹œë‚˜ë¦¬ì˜¤**:

1. **Critical ê°ì§€ ì‹œ**:
   - MQTT ì¦‰ì‹œ ì•Œë¦¼
   - (ì„ íƒ) í”„ë¦°í„° ì¼ì‹œì •ì§€
   - ê´€ë¦¬ìì—ê²Œ SMS/Email ì „ì†¡

2. **High ê°ì§€ ë°˜ë³µ ì‹œ** (5ë¶„ ë‚´ 3íšŒ):
   - ê²½ê³  ì•Œë¦¼
   - ëŒ€ì‹œë³´ë“œì— ê²½ê³  í‘œì‹œ

3. **Medium ê°ì§€**:
   - ë¡œê·¸ ê¸°ë¡
   - í†µê³„ì—ë§Œ ë°˜ì˜

---

## ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### ë³´ì•ˆ
- [ ] API ì¸ì¦ (JWT)
- [ ] MQTT TLS ì•”í˜¸í™”
- [ ] ì´ë¯¸ì§€ ë°ì´í„° ì•”í˜¸í™”

### í™•ì¥ì„±
- [ ] ë©€í‹° GPU ì§€ì›
- [ ] ë¡œë“œ ë°¸ëŸ°ì‹±
- [ ] Redis ìºì‹±

### ë¹„ìš© ìµœì í™”
- [ ] ëª¨ë¸ ê²½ëŸ‰í™” (Pruning, Quantization)
- [ ] í´ë¼ìš°ë“œ vs ì˜¨í”„ë ˆë¯¸ìŠ¤ ì„ íƒ
- [ ] Spot Instance í™œìš©

---

## ì°¸ê³  ìë£Œ

### AI ëª¨ë¸
- [Ultralytics YOLOv8 Docs](https://docs.ultralytics.com/)
- [Florence-2 Hugging Face](https://huggingface.co/microsoft/Florence-2-base)
- [SAM2 GitHub](https://github.com/facebookresearch/segment-anything-2)

### 3D í”„ë¦°í„° ë¶ˆëŸ‰ ê°ì§€
- [The Spaghetti Detective](https://www.thespaghettidetective.com/)
- [OctoPrint AI Plugin](https://plugins.octoprint.org/)

### ë°ì´í„°ì…‹
- [Kaggle 3D Print Dataset](https://www.kaggle.com/datasets)
- [Roboflow 3D Printing](https://universe.roboflow.com/)

---

## íƒ€ì„ë¼ì¸ ìš”ì•½

| Week | Phase | ì£¼ìš” ì‘ì—… | ì‚°ì¶œë¬¼ |
|------|-------|----------|--------|
| 1-2 | Phase 1 | ì¸í”„ë¼ êµ¬ì¶• | WebRTC ìº¡ì²˜, DB ìŠ¤í‚¤ë§ˆ |
| 3-4 | Phase 2 | AI ëª¨ë¸ í•™ìŠµ | YOLOv8 í•™ìŠµ ëª¨ë¸ |
| 5-6 | Phase 3 | ì¶”ë¡  ì„œë¹„ìŠ¤ | AI API ì—”ë“œí¬ì¸íŠ¸ |
| 7 | Phase 4 | ì›Œì»¤ ê°œë°œ | ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ |
| 8 | Phase 5 | í”„ë¡ íŠ¸ì—”ë“œ | ëŒ€ì‹œë³´ë“œ UI |
| 9-10 | Phase 6 | í…ŒìŠ¤íŠ¸ | ì„±ëŠ¥/ì •í™•ë„ ë³´ê³ ì„œ |
| 11-12 | Phase 7 | ë°°í¬ | Docker, ëª¨ë‹ˆí„°ë§ |

**ì´ ì˜ˆìƒ ê¸°ê°„**: 12ì£¼ (ì•½ 3ê°œì›”)

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê°œë°œ í™˜ê²½
- [ ] Python 3.11 ì„¤ì¹˜
- [ ] CUDA & cuDNN ì„¤ì¹˜ (GPU ì‚¬ìš© ì‹œ)
- [ ] PyTorch ì„¤ì¹˜
- [ ] Ultralytics ì„¤ì¹˜
- [ ] OpenCV ì„¤ì¹˜

### ë°ì´í„°
- [ ] í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ (ìµœì†Œ 1000ì¥)
- [ ] ë°ì´í„° ë¼ë²¨ë§ (Roboflow)
- [ ] ë°ì´í„°ì…‹ ë¶„í•  (train/val/test)

### ëª¨ë¸
- [ ] YOLOv8 í•™ìŠµ
- [ ] ëª¨ë¸ ê²€ì¦ (mAP > 0.7)
- [ ] ëª¨ë¸ ìµœì í™” (ONNX ë³€í™˜)

### API
- [ ] ëª¨ë‹ˆí„°ë§ API êµ¬í˜„
- [ ] WebRTC í†µí•©
- [ ] MQTT ì•Œë¦¼ êµ¬í˜„
- [ ] DB ì—°ë™

### í”„ë¡ íŠ¸ì—”ë“œ
- [ ] AI ëŒ€ì‹œë³´ë“œ êµ¬í˜„
- [ ] ì‹¤ì‹œê°„ ì•Œë¦¼ UI
- [ ] ê°ì§€ ì´ë ¥ ì°¨íŠ¸

### ë°°í¬
- [ ] Docker ì´ë¯¸ì§€ ë¹Œë“œ
- [ ] GPU ì„¤ì •
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ (Grafana)
- [ ] ë¡œê·¸ ìˆ˜ì§‘ (ELK Stack)

---

**ë¬¸ì„œ ì‘ì„±ì¼**: 2025-01-26
**ìµœì¢… ìˆ˜ì •ì¼**: 2025-01-26
**ì‘ì„±ì**: Claude AI Assistant
